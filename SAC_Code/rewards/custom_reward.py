import numpy as np
from citylearn.reward_function import RewardFunction

class ComfortandConsumptionReductionReward(RewardFunction):
    """
    Simple reward that directly mirrors CityLearn competition scoring.
    Weights match the actual competition: comfort 30%, grid 30%, carbon 10%.
    """
    def __init__(self, agent_count, comfort_band=2.0):
        super().__init__(agent_count)
        self.comfort_band = comfort_band
        
        # Track previous values for ramping calculation
        self._prev_net = None if agent_count is None else [None for _ in range(agent_count)]
        
        # Competition-aligned weights (Phase I from README)
        self.comfort_weight = 0.3    # 30% - Unmet hours weight
        self.carbon_weight = 0.1     # 10% - Carbon emissions weight  
        self.ramping_weight = 0.075  # 7.5% - Ramping weight
        self.load_factor_weight = 0.075  # 7.5% - Load factor weight
        self.daily_peak_weight = 0.075   # 7.5% - Daily peak weight
        self.alltime_peak_weight = 0.075 # 7.5% - All-time peak weight
        
    def calculate(self, observations):
        # Lazy initialization if agent_count was None
        if self._prev_net is None:
            agent_count = len(observations)
            self._prev_net = [None for _ in range(agent_count)]
        
        rewards = []
        
        for i, o in enumerate(observations):
            # Start with positive baseline
            reward = 1.0
            
            # 1. COMFORT PENALTY (30% weight in competition)
            Tin = float(o.get('indoor_dry_bulb_temperature', 22.0))
            if 'indoor_dry_bulb_temperature_set_point' in o:
                Tsp = float(o['indoor_dry_bulb_temperature_set_point'])
            else:
                Tsp = 22.0
            
            temp_violation = abs(Tin - Tsp)
            if temp_violation > self.comfort_band:
                comfort_penalty = (temp_violation - self.comfort_band) * self.comfort_weight
            else:
                comfort_penalty = 0.0
            
            # 2. GRID PENALTIES (30% total weight: ramping + load factor + peaks)
            net = float(o.get('net_electricity_consumption', 0.0))
            
            # Ramping penalty (7.5% weight)
            ramping_penalty = 0.0
            if self._prev_net[i] is not None:
                ramping = abs(net - self._prev_net[i])
                # Normalize ramping (typical range 0-1, target: minimize changes)
                ramping_penalty = ramping * self.ramping_weight
            
            self._prev_net[i] = net
            
            # 3. CARBON PENALTY (10% weight) - approximate
            # Use consumption as proxy for carbon (real carbon = consumption Ã— intensity)
            carbon_penalty = abs(net) * self.carbon_weight * 0.1  # Scale down
            
            # Calculate competition-style reward (minimize penalties)
            final_reward = reward - comfort_penalty - ramping_penalty - carbon_penalty
            
            rewards.append(final_reward)
        
        return np.array(rewards, dtype='float32')
